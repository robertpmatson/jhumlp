---
title: "Practical Machine Learning - Course Project"
author: "Robert Matson"
date: "February 28, 2016"
output: html_document
---

##Synopsis

#####Quoted directly from the project outline

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

###Data

We have downloaded the data for the training and test data sets to our working directory. 

```{r}
trainingDataCsv <- "trainingData.csv"
testingDataCsv <- "testing.csv"
trainingDataSource <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testingDataSource <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists(trainingDataCsv)){ download.file(trainingDataSource, destfile=trainingDataCsv) }
if (!file.exists(testingDataCsv)){ download.file(testingDataSource, destfile=testingDataCsv) }
```

The training data set for this project is available here:  
`r trainingDataSource`

The test data set is available here:  
`r testingDataSource`

Using this data we will use the training dta set to build a model which will be used to predict the "classe" label of the test data set. This is a predictione exercise and the type of prdediction is "Classification".

###Required libraries

The following libraries are requiuired and assumed to be installed. Additionally the version of caret is shown.

```{r, message=FALSE}
library("caret")
library("data.table")
library("arm")
library("doMC")
registerDoMC(cores = 5)
packageVersion("caret")
```

###Download data and initial exploration

First we use the "fread" function from the "data.table" package to read the data, defining blank strings and the value "NA" as the 'NA' strings. 

```{r}
trainingDataSet <- fread(trainingDataCsv, na.strings=c("", "NA"))
testingDataSet <- fread(testingDataCsv, na.strings=c("", "NA"))
# Convert classe to a Factor variable
trainingDataSet$classe <- as.factor(trainingDataSet$classe)
```

The data is clean: variables are in a single column, with each observation in a single row. The data structure is both large and sparse. Analysing the data structure in detail would make the project excessively large in terms of the number of pages, so for brevity the specifics of this analysis are not included. 

The Response variable "classe" has a distributionas shown below. Class A is the most frequent level and the remaining 4 levels appear to be within a close range meaning that the classes are relatively balanced. 

```{r, warning=FALSE, message=FALSE}
plot(trainingDataSet$classe, col="darkgreen", main='Distribution of the Levels of the response variable classe', xlab="Factor level for classe", ylab="Frequency", type="h")
rowCount <- nrow(trainingDataSet)
table(trainingDataSet$classe)
naSumCutOff <- 0.9
```

We will get rid of columns that have a large proportion (over `r naSumCutOff * 100`%) of "NA" values and where we cannot infer any reasonable value. The first seven columns (user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window and num_window) are date/time or indentification values and in my opinion are not relevant to any predictive model and therefore are removed.

```{r, cache=TRUE}
nonSparseColumns <- colSums(is.na(trainingDataSet)) <= nrow(trainingDataSet) * naSumCutOff
names <- names(trainingDataSet)
names[1:7]
irrelevantColumns <- c(1:7)
#Convert to a data frame and then select the columns
trainingDataSet <- data.frame(trainingDataSet)[ , nonSparseColumns]
#Remove the first 7 columns, because this data is irrelevant (Name, Date/Time, row index, ...)
trainingDataSet <- trainingDataSet[ , -irrelevantColumns]
testingDataSet <- data.frame(testingDataSet)[ , nonSparseColumns]
testingDataSet <- testingDataSet[ , -irrelevantColumns]
```

The number of columns removed because of sparse data is `r length(which(nonSparseColumns == TRUE))`. Some attributes may have a low variance, which means that the attribute values will be nearly the same for all observations. Low variance data is not very useful for predictive models and should be removed. The caret package provides us with a useful utility to find any such attributes and from the data that has already been cleaned already there appear to be no low variance variables remaining.

```{r}
lowVariance <- nearZeroVar(trainingDataSet, saveMetrics=T)
lowVariance[lowVariance$nzv == TRUE,]
trainingDataSet <- trainingDataSet[, !(lowVariance$nzv)]
testingDataSet <- testingDataSet[, !(lowVariance$nzv)]
corrCutoff <- 0.8
```

We will also check for any predictors that are highly correlated so we will remove the highly correlated predictors, those with absolute correlations above `r corrCutoff ` . Variables that are highly correlated tend to change together so changing the value in one variable would usually mean a change in highly correlated variable.

```{r}
# classe is the last attribute, remove it for the correlation function
colCount <- dim(trainingDataSet)[2]
trainingCorr <- cor(trainingDataSet[, -colCount])
highlyCorPred <- findCorrelation(trainingCorr, cutoff=corrCutoff)
trainingDataSet <- trainingDataSet[,-highlyCorPred]
trainingDataSet <- trainingDataSet[,-highlyCorPred]

trainValidationSplit <- 0.75
numberOfFolds <- 10
numberOfRepeates <- 5
```

###Partition the Data 

Now we will partition our data into a training and validation set. We will take `r trainValidationSplit * 100`% of our data for training the model and the remaining `r (1- trainValidationSplit) * 100`% for model validation. We will split over the response variable to get an equal proportion of "classe" values in each set.

```{r}
set.seed(8020)
trainIndex <- createDataPartition(trainingDataSet$classe, p=trainValidationSplit, list=FALSE, times=1)
trainingSet <- trainingDataSet[trainIndex,]
validationSet <- trainingDataSet[-trainIndex,]
```

###Finding our Model

We will use `r numberOfFolds` fold cross-validation which is repeated `r numberOfRepeates` times when training our model. We are using parallel processing provided by the "doMC" library to gain better performance when creating our model. 

To train our model and to compare two different models, we will try with a boosted model and then we will use a Random Forest model. The are both models widely used for classification problems such as the prediction problem that we are trying to solve. 

```{r, cache=TRUE}
fitControl <- trainControl(method="cv", number=10, repeats = 3)
```

1. Generalised Boosted Model (GBM). We use the caret package to train the model, the method argument will use the "gbm" method to build our model.

```{r, cache=TRUE, message=FALSE}
gbm.fit <- train(classe ~ . , data=trainingSet,
                 method="gbm", trControl=fitControl, verbose=FALSE)
gbm.fit
```

2. Random Forest model. Again using the caret package, we give the method argument as "rf", to se a Random Forest to build our model.

```{r, cache=TRUE, message=FALSE}
rf.fit1 <- train(classe ~ . , data=trainingSet,
                 method="rf", trControl=fitControl, verbose=FALSE)
rf.fit1
```

Random Forest gives the highest prediction accuracy. Let us see how both models do on the validation data set, which is the data that we took for validating the model created using the training data.

```{r, cache=TRUE, message=FALSE}
gbmPredict <- predict(gbm.fit, validationSet)
rfPredict1 <- predict(rf.fit1, validationSet)
```

###Prediction performance using the Validation Data Set

```{r}
gbmPredict.cm <- confusionMatrix(gbmPredict, validationSet$classe)
gbmPredictAccuracy <- round(gbmPredict.cm$overall["Accuracy"],4)
rfPredict1.cm <- confusionMatrix(rfPredict1, validationSet$classe)
rfPredictAccuracy <- round(rfPredict1.cm$overall["Accuracy"], 4)
```

The GBM model gives an accuracy of `r gbmPredictAccuracy * 100 `% on the test data, which is a high accuracy rate. Random guessing would have a rate of 0.2, so our model performs considerably better.

```{r}
gbmPredict.cm
```

However it was not the best model. The Random Forest model achieved an accuracy of `r rfPredictAccuracy * 100 `% on the test data. The confusion matrix below shows that here were very few false classifications, and this is the better model. 

```{r, cache=TRUE, message=FALSE}
rfPredict1.cm
```

```{r, echo=FALSE}
rfPredict.ciLower <- round(rfPredict1.cm$overall["AccuracyLower"],4)
rfPredict.ciUpper <- round(rfPredict1.cm$overall["AccuracyUpper"],4)
```

With a 95% confidence interval of classifying between `r rfPredict.ciLower * 100 ` and `r rfPredict.ciUpper * 100` of the classes correctly, this is a very high rate. 

###Can we simplify the model ?

We do have a model that has potentially many predictors and we might be able to simplify this model and keep a similar accuracy. Below we show the variable importance and by removing less important parameters maybe we could find a more parsimonious model.
 
```{r, cache=TRUE, message=FALSE}
importanceThreshold <- 20
predictorImportance <- varImp(rf.fit1)
plot(predictorImportance)
varImportance <- predictorImportance[[1]]
importantVariables <- rownames(varImportance)[order(rowSums(varImportance), decreasing=TRUE)][rowSums(varImportance) > importanceThreshold]
simpleCount <- length(importantVariables)
importantVariables <- c(importantVariables, "classe")
```

Now that we have the most important variables, we will generate a new model model. There are several predictors with a low important so to make the model more understandable we can remove the low importance variables and check the accuracy. I have selected the `r simpleCount` most important parameters, based on having a scaled importance of more than `r importanceThreshold` and we will run the Random Forest model generation using these variables.

```{r, cache=TRUE, message=FALSE}
trainingSet2 <- trainingSet[, importantVariables]
rf.fit2 <- train(classe ~ ., data=trainingSet2,
                 method="rf", trControl=fitControl, verbose=FALSE)
rf.fit2
```

Let us see how well it predicts using the validation set.

```{r, message=FALSE}
predict.rf.2 <- predict(rf.fit2, validationSet)
predict.rf.2.cm <- confusionMatrix(predict.rf.2, validationSet$classe)
predict.rf.2.cm

```
```{r, echo=FALSE}
rf2.PredictAccuracy <- round(predict.rf.2.cm$overall["Accuracy"], 4)
rf2.Predict.ciLower <- round(predict.rf.2.cm$overall["AccuracyLower"],4)
rf2.Predict.ciUpper <- round(predict.rf.2.cm$overall["AccuracyUpper"],4)
```

This gives us an accuracy value of `r rf2.PredictAccuracy`, and a 95% CI of ana ccuracy level of between `r rf2.Predict.ciLower` and `r rf2.Predict.ciUpper`. This slightly simpler model gives us a similar but a slighty lower accuracy rate when compared to the optimum model found earlier.


###Out-of-Sample Error

The out-of-sample error rate is the error rate of the model when using the model generated from the training data to predict the response variable in the validation data set. This is a value of 1 minus the accuracy

The out of sample error is calculated below, based on the predictions generated by the Random Forest model using the validation data set that we created earlier.

Out of sample accuracy is:

```{r, message=FALSE, warning=FALSE}
rf2.PredictAccuracy
```

The expected out of sample error rate is 1 minus `r rf2.PredictAccuracy`, which converts as a percentage to `r (1 - rf2.PredictAccuracy) * 100`%. Based on this expected out-of-sample error rate we can expect approximately one of the 20 predictions in the quiz to be incorrect.

###Using the test data to make predictions
Now using the test set, and our optimum Random Forest prediction model, we get the predictions below for the first 20 elements of the test data, using the Random Forest model with the highest accuracy.

```{r}
predict.rf.1.test <- predict(rf.fit1, testingDataSet)
predict.rf.1.test[1:20]
```

### Conclusion

Based on the two generated models, we have concluded that the Random Forst model will give a better prediction accuracy and a lower out-of-sample error rate.